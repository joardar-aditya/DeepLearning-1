{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Analysis using Logistic Regression",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNflfue3wF2+Xmo4JchGC+i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joardar-aditya/DeepLearning-1/blob/master/Twitter_Sentiment_Analysis_using_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9FKQcj9iLeZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5addcae2-23aa-4d2a-8738-75d130e7411c"
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDklxxDTjkjO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9d79d8f3-067d-4336-e43c-1d4bb741ccaf"
      },
      "source": [
        "#import nltk tweets dataset \n",
        "\n",
        "\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn7jt2tYmADv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzjSh3esn_Sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating datatset for the testing and training \n",
        "test_set_pos = all_positive_tweets[4000:]\n",
        "train_set_pos = all_positive_tweets[:4000]\n",
        "test_set_neg = all_negative_tweets[4000:]\n",
        "train_set_neg = all_negative_tweets[:4000]\n",
        "\n",
        "test_x = test_set_pos + test_set_neg\n",
        "train_x  = train_set_pos + train_set_neg \n",
        "\n",
        "test_y = np.append(np.ones((len(test_set_pos), 1)), np.zeros((len(test_set_neg), 1)), axis=0)\n",
        "train_y = np.append(np.ones((len(train_set_pos), 1)), np.zeros((len(train_set_neg), 1)), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNXGPv4NqOEj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3023fdf4-31e7-4caf-e4bd-af15d079515e"
      },
      "source": [
        "test_x[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Bro:U wan cut hair anot,ur hair long Liao bo\\nMe:since ord liao,take it easy lor treat as save $ leave it longer :)\\nBro:LOL Sibei xialan'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FySM3mrstHgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' process tweet -: only stopwords, remove punctuations and for the frequency matrix \n",
        "Three important concepts -: \n",
        "   1) Tokenizing \n",
        "   2) Stemming \n",
        "   3) Lower casting '''\n",
        "import re\n",
        "import string \n",
        "\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import TweetTokenizer \n",
        "\n",
        "\n",
        "#Process an individual tweet \n",
        "\n",
        "stop_words_english = stopwords.words('english')\n",
        "\n",
        "def process_tweet(tweet):\n",
        "  tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "  tweet3 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
        "  tweet3 = re.sub(r'#', '', tweet3)\n",
        "  \n",
        "  tokenizer = TweetTokenizer(preserve_case =False, strip_handles = True, reduce_len = True)\n",
        "\n",
        "  tweet_tokens = tokenizer.tokenize(tweet3)\n",
        "  #Tokenizing Done\n",
        "  tweet_words = []\n",
        "  for words in tweet_tokens:\n",
        "    if (words not in stop_words_english and words not in string.punctuation):\n",
        "          tweet_words.append(words)\n",
        "\n",
        "  #Clean Up Done\n",
        "  tweet_stem = []\n",
        "  stemmer = PorterStemmer()\n",
        "  for words in tweet_words:\n",
        "        stem_word = stemmer.stem(words)\n",
        "        tweet_stem.append(stem_word)\n",
        "  #Stemming Done \n",
        "  \n",
        "  return tweet_stem\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QJ19uN38IMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating Frequency table\n",
        "frequency_table = {}\n",
        "\n",
        "for tweet in test_set_pos + train_set_pos: \n",
        "   tweet_stem = process_tweet(tweet)\n",
        "   for word in tweet_stem:\n",
        "     if (word, 1) in frequency_table.keys():\n",
        "         frequency_table[(word,1)] += 1\n",
        "     else:\n",
        "         frequency_table[(word,1)] = 1\n",
        "\n",
        "for tweet in test_set_neg + train_set_neg: \n",
        "   tweet_stem = process_tweet(tweet)\n",
        "   for word in tweet_stem:\n",
        "     if (word, 0) in frequency_table.keys():\n",
        "         frequency_table[(word,0)] += 1\n",
        "     else:\n",
        "         frequency_table[(word,0)] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEk3BuRn-1-q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3ab992b-a812-4702-b6c8-c8415177eb8b"
      },
      "source": [
        "''' PROCESSING FINISHED '''\n",
        "\n",
        "\n",
        "''' Logistic Regression Implementation '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Logistic Regression Implementation '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O61Htsh-EVL4",
        "colab_type": "text"
      },
      "source": [
        "Gradient Descent Explanation Tweet -: https://twitter.com/joardar7/status/1292144626795593729?s=20\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwlfUERQp2d8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "\n",
        "  h = 1/(1 + np.exp(-z))\n",
        "\n",
        "  return h \n",
        "\n",
        "no_of_iter = 600 #More iters , more training \n",
        "\n",
        "alpha = 0.00001 #Hyperparamter, smaller the better but don't make it too small. \n",
        "\n",
        "def gradientdescent(x , y, alpha, theta, no_of_iter):\n",
        "    \n",
        "    m = x.shape[0]\n",
        "    \n",
        "    # x shape (m, n + 1) and theta  shape (n + 1 , 1) , z shape -: (m , 1), y shape is (m, 1)\n",
        "    for i in range(no_of_iter):\n",
        "      \n",
        "      z = np.dot(x,theta) \n",
        "\n",
        "      h = sigmoid(z)\n",
        "\n",
        "      ''' Cost function is the difference between the predicted value and the actual value of prediction, \n",
        "      Now the fun part -: \n",
        "\n",
        "      Cost Function for the logistic function is -: \n",
        "\n",
        "      J = -1/m * sum( y*log(h) + (1-y)*log(1 - h))\n",
        "\n",
        "      WHy this is fun, the y is always 0 or 1 so if y = 0  and h is - then y = 0 for first term and log1 = 0 for second which makes error zero. \n",
        "\n",
        "      Remember it as the cost function of the logistic regression (every logistic regression)\n",
        "      \n",
        "      x remains unchanged \n",
        "\n",
        "      \n",
        "      '''\n",
        "\n",
        "      J = -1/m * (np.dot(y.transpose(), np.log(h)) + np.dot(1 - y.transpose(),np.log(1 - h))) \n",
        "    \n",
        "     \n",
        "      theta = theta - alpha/m * np.dot(x.transpose(), (h - y))\n",
        "\n",
        "    J = float(J)\n",
        "\n",
        "    return J, theta  \n",
        "  \n",
        "      \n",
        "# make x -: feature set  \n",
        "\n",
        "def extract_feature(tweet, freq):\n",
        "\n",
        "  x = np.zeros((1,3))\n",
        "\n",
        "  ntweet = process_tweet(tweet)\n",
        "\n",
        "  x[0,0] = 1 #(bias term)\n",
        "\n",
        "  for word in ntweet :\n",
        "      \n",
        "      x[0, 1] += freq.get((word, 1), 0)\n",
        "\n",
        "      x[0, 2] += freq.get((word, 0), 0)\n",
        "\n",
        "  return x \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVCxPG4MOw7q",
        "colab_type": "text"
      },
      "source": [
        "##Training begins -: \n",
        "\n",
        "Form the x, get the Y and give the theta (all 0, always has been); \n",
        "\n",
        "do gradient descent, and then get the prediction using the theta. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xGnRBkVRA9j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "27e5212e-2191-41b5-d43f-f39c20e4e118"
      },
      "source": [
        "X_train = np.zeros((len(train_x), 3))\n",
        "\n",
        "for i in range(len(train_x)):\n",
        "\n",
        "  X_train[i, :] = extract_feature(train_x[i], frequency_table) \n",
        "\n",
        "\n",
        "\n",
        "J, theta = gradientdescent(X_train, train_y, alpha, np.zeros((3, 1)), no_of_iter)\n",
        "\n",
        "print(J)\n",
        "print(f\"theta value :{[round(t, 8) for t in np.squeeze(theta)]}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "nan\n",
            "theta value :[3.122e-05, 0.0083235, -0.007581]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbtPxAJ2Uyik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_tweet(tweet, theta, freq):\n",
        "  X = extract_feature(tweet, freq)\n",
        "  h = sigmoid(np.dot(X, theta))\n",
        "  return h "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0GUxIU0WmJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d96884c-8a35-49dc-d902-34c09c55fe7f"
      },
      "source": [
        "predict_tweet('I love you', theta, frequency_table)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8981759]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFDwmLnLWuCN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d8fdbbaf-11a9-461d-d676-fc9622facb1b"
      },
      "source": [
        "# Testing accuracy \n",
        "correct = 0\n",
        "for i in range(len(test_x)):\n",
        "\n",
        "   result = predict_tweet(test_x[i], theta, frequency_table)\n",
        "\n",
        "   value = 0\n",
        "   if(float(result) > 0.5):\n",
        "     value = 1\n",
        "   if(value == test_y[i]):\n",
        "     correct += 1\n",
        "\n",
        "print(\"Accuracy -: \")\n",
        "\n",
        "print(correct/len(test_x))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy -: \n",
            "0.9955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI3h_94NcLaQ",
        "colab_type": "text"
      },
      "source": [
        "# We are happy with a 99.5 % Accuracy! "
      ]
    }
  ]
}